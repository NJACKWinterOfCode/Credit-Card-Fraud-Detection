{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import  seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62    0.0  \n",
       "1  0.125895 -0.008983  0.014724    2.69    0.0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66    0.0  \n",
       "3 -0.221929  0.062723  0.061458  123.50    0.0  \n",
       "4  0.502292  0.219422  0.215153   69.99    0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"Dataset/creditcard.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284617, 31)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      4668\n",
       "V1        4668\n",
       "V2        4668\n",
       "V3        4668\n",
       "V4        4668\n",
       "V5        4668\n",
       "V6        4668\n",
       "V7        4668\n",
       "V8        4668\n",
       "V9        4668\n",
       "V10       4668\n",
       "V11       4668\n",
       "V12       4668\n",
       "V13       4668\n",
       "V14       4668\n",
       "V15       4668\n",
       "V16       4668\n",
       "V17       4668\n",
       "V18       4668\n",
       "V19       4668\n",
       "V20       4668\n",
       "V21       4668\n",
       "V22       4668\n",
       "V23       4668\n",
       "V24       4668\n",
       "V25       4668\n",
       "V26       4668\n",
       "V27       4668\n",
       "V28       4668\n",
       "Amount    4668\n",
       "Class     4668\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    279461\n",
       "1.0       488\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((279461, 31), (488, 31))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud=data[data['Class']==0]\n",
    "non_fraud=data[data['Class']==1]\n",
    "fraud.shape,non_fraud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279944</th>\n",
       "      <td>166028.0</td>\n",
       "      <td>-0.956390</td>\n",
       "      <td>2.361594</td>\n",
       "      <td>-3.171195</td>\n",
       "      <td>1.970759</td>\n",
       "      <td>0.474761</td>\n",
       "      <td>-1.902598</td>\n",
       "      <td>-0.055178</td>\n",
       "      <td>0.277831</td>\n",
       "      <td>-1.745854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473211</td>\n",
       "      <td>0.719400</td>\n",
       "      <td>0.122458</td>\n",
       "      <td>-0.255650</td>\n",
       "      <td>-0.619259</td>\n",
       "      <td>-0.484280</td>\n",
       "      <td>0.683535</td>\n",
       "      <td>0.443299</td>\n",
       "      <td>39.90</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279945</th>\n",
       "      <td>166831.0</td>\n",
       "      <td>-2.027135</td>\n",
       "      <td>-1.131890</td>\n",
       "      <td>-1.135194</td>\n",
       "      <td>1.086963</td>\n",
       "      <td>-0.010547</td>\n",
       "      <td>0.423797</td>\n",
       "      <td>3.790880</td>\n",
       "      <td>-1.155595</td>\n",
       "      <td>-0.063434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315105</td>\n",
       "      <td>0.575520</td>\n",
       "      <td>0.490842</td>\n",
       "      <td>0.756502</td>\n",
       "      <td>-0.142685</td>\n",
       "      <td>-0.602777</td>\n",
       "      <td>0.508712</td>\n",
       "      <td>-0.091646</td>\n",
       "      <td>634.30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279946</th>\n",
       "      <td>166883.0</td>\n",
       "      <td>2.091900</td>\n",
       "      <td>-0.757459</td>\n",
       "      <td>-1.192258</td>\n",
       "      <td>-0.755458</td>\n",
       "      <td>-0.620324</td>\n",
       "      <td>-0.322077</td>\n",
       "      <td>-1.082511</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>-0.140927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288253</td>\n",
       "      <td>0.831939</td>\n",
       "      <td>0.142007</td>\n",
       "      <td>0.592615</td>\n",
       "      <td>-0.196143</td>\n",
       "      <td>-0.136676</td>\n",
       "      <td>0.020182</td>\n",
       "      <td>-0.015470</td>\n",
       "      <td>19.95</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279947</th>\n",
       "      <td>167338.0</td>\n",
       "      <td>-1.374424</td>\n",
       "      <td>2.793185</td>\n",
       "      <td>-4.346572</td>\n",
       "      <td>2.400731</td>\n",
       "      <td>-1.688433</td>\n",
       "      <td>0.111136</td>\n",
       "      <td>-0.922038</td>\n",
       "      <td>-2.149930</td>\n",
       "      <td>-2.027474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.870779</td>\n",
       "      <td>0.504849</td>\n",
       "      <td>0.137994</td>\n",
       "      <td>0.368275</td>\n",
       "      <td>0.103137</td>\n",
       "      <td>-0.414209</td>\n",
       "      <td>0.454982</td>\n",
       "      <td>0.096711</td>\n",
       "      <td>349.08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279948</th>\n",
       "      <td>169142.0</td>\n",
       "      <td>-1.927883</td>\n",
       "      <td>1.125653</td>\n",
       "      <td>-4.518331</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>-1.566487</td>\n",
       "      <td>-2.010494</td>\n",
       "      <td>-0.882850</td>\n",
       "      <td>0.697211</td>\n",
       "      <td>-2.064945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778584</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>0.639419</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>0.537503</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.147968</td>\n",
       "      <td>390.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279949 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0            0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1            0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2            1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3            1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4            2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "279944  166028.0 -0.956390  2.361594 -3.171195  1.970759  0.474761 -1.902598   \n",
       "279945  166831.0 -2.027135 -1.131890 -1.135194  1.086963 -0.010547  0.423797   \n",
       "279946  166883.0  2.091900 -0.757459 -1.192258 -0.755458 -0.620324 -0.322077   \n",
       "279947  167338.0 -1.374424  2.793185 -4.346572  2.400731 -1.688433  0.111136   \n",
       "279948  169142.0 -1.927883  1.125653 -4.518331  1.749293 -1.566487 -2.010494   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0       0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
       "1      -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "2       0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n",
       "3       0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
       "4       0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "279944 -0.055178  0.277831 -1.745854  ...  0.473211  0.719400  0.122458   \n",
       "279945  3.790880 -1.155595 -0.063434  ... -0.315105  0.575520  0.490842   \n",
       "279946 -1.082511  0.117200 -0.140927  ...  0.288253  0.831939  0.142007   \n",
       "279947 -0.922038 -2.149930 -2.027474  ... -0.870779  0.504849  0.137994   \n",
       "279948 -0.882850  0.697211 -2.064945  ...  0.778584 -0.319189  0.639419   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "0       0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62    0.0  \n",
       "1      -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69    0.0  \n",
       "2      -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66    0.0  \n",
       "3      -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50    0.0  \n",
       "4       0.141267 -0.206010  0.502292  0.219422  0.215153   69.99    0.0  \n",
       "...          ...       ...       ...       ...       ...     ...    ...  \n",
       "279944 -0.255650 -0.619259 -0.484280  0.683535  0.443299   39.90    1.0  \n",
       "279945  0.756502 -0.142685 -0.602777  0.508712 -0.091646  634.30    1.0  \n",
       "279946  0.592615 -0.196143 -0.136676  0.020182 -0.015470   19.95    1.0  \n",
       "279947  0.368275  0.103137 -0.414209  0.454982  0.096711  349.08    1.0  \n",
       "279948 -0.294885  0.537503  0.788395  0.292680  0.147968  390.00    1.0  \n",
       "\n",
       "[279949 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fraud.append(non_fraud, ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279949, 30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop('Class', axis = 1)\n",
    "y = data['Class']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279949,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=shuffle(data)\n",
    "data_majority = data[data.Class==0]\n",
    "data_minority = data[data.Class==1]\n",
    "data_minority_upsampled = resample(data_minority, \n",
    "                                 replace=True,     \n",
    "                                 n_samples=100000,    \n",
    "                                 random_state=123)\n",
    "data_upsampled = pd.concat([data_majority, data_minority_upsampled])\n",
    "features=data_upsampled.iloc[:,:-1]\n",
    "labels=data_upsampled.iloc[:,30:31]\n",
    "features,labels=shuffle(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((265622, 30), (113839, 30))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features,labels,test_size=0.3,random_state=123,stratify=labels)\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((265622, 30), (113839, 30))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((265622, 30, 1), (113839, 30, 1))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, 2, activation='relu', input_shape = X_train[0].shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(64, 2, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_24 (Conv1D)           (None, 29, 32)            96        \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 29, 32)            128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 13, 64)            4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 13, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                24640     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 29,345\n",
      "Trainable params: 29,153\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 265622 samples, validate on 113839 samples\n",
      "Epoch 1/10\n",
      "265622/265622 [==============================] - 350s 1ms/sample - loss: 0.0857 - accuracy: 0.9701 - f1_m: 0.9374 - precision_m: 0.9678 - recall_m: 0.9164 - val_loss: 0.0330 - val_accuracy: 0.9895 - val_f1_m: 0.9784 - val_precision_m: 0.9858 - val_recall_m: 0.9739\n",
      "Epoch 2/10\n",
      "265622/265622 [==============================] - 360s 1ms/sample - loss: 0.0596 - accuracy: 0.9784 - f1_m: 0.9556 - precision_m: 0.9737 - recall_m: 0.9436 - val_loss: 0.0243 - val_accuracy: 0.9932 - val_f1_m: 0.9860 - val_precision_m: 0.9865 - val_recall_m: 0.9873\n",
      "Epoch 3/10\n",
      "265622/265622 [==============================] - 352s 1ms/sample - loss: 0.0517 - accuracy: 0.9812 - f1_m: 0.9615 - precision_m: 0.9753 - recall_m: 0.9530 - val_loss: 0.0197 - val_accuracy: 0.9958 - val_f1_m: 0.9914 - val_precision_m: 0.9884 - val_recall_m: 0.9956\n",
      "Epoch 4/10\n",
      "265622/265622 [==============================] - 364s 1ms/sample - loss: 0.0492 - accuracy: 0.9825 - f1_m: 0.9645 - precision_m: 0.9779 - recall_m: 0.9558 - val_loss: 0.0185 - val_accuracy: 0.9955 - val_f1_m: 0.9907 - val_precision_m: 0.9871 - val_recall_m: 0.9956\n",
      "Epoch 5/10\n",
      "265622/265622 [==============================] - 372s 1ms/sample - loss: 0.0443 - accuracy: 0.9840 - f1_m: 0.9674 - precision_m: 0.9796 - recall_m: 0.9597 - val_loss: 0.0171 - val_accuracy: 0.9955 - val_f1_m: 0.9907 - val_precision_m: 0.9888 - val_recall_m: 0.99399 - accuracy: 0.9841 - f1_m: 0.96 - ETA: 22s - loss: 0.0440 - accuracy: 0.9841 - f - ETA: 13s - loss: 0.0441 - accuracy: 0.9840 - ETA: 11s - loss: 0.0442 - accuracy: 0.9840 - f1_m: 0.9673 - precision_m: 0.9796 - recall_ - ETA: 10s - loss: 0.0442 - accuracy: 0.9840 - f1_m: 0.9673 - precis - ETA: 3s - loss: 0.0444 - accuracy: 0.9840 - f1_m: 0.9673 - precision_m: 0.9795 -  - ETA: 3s - loss: 0.0444 - accuracy: 0.9840 - f1_m: 0.9673  - ETA: 2s - loss: 0.0443 - accuracy: 0.9840 - f1_m: 0.9674  - ETA: 1s - loss: 0.0443 - accuracy: 0. - ETA: 0s - loss: 0.0443 - accuracy: 0.9840 - f1_m: 0.9674 - precision_m: 0.9796 - recall_m:  - ETA: 0s - loss: 0.0443 - accuracy: 0.9840 - f1_m: 0.9674 - precision_m: 0.9796 - recall_m: 0.95 - ETA: 0s - loss: 0.0443 - accuracy: 0.9840 - f1_m: 0.9674 - precision_m: 0.9796 - reca\n",
      "Epoch 6/10\n",
      "265622/265622 [==============================] - 429s 2ms/sample - loss: 0.0424 - accuracy: 0.9847 - f1_m: 0.9690 - precision_m: 0.9797 - recall_m: 0.9625 - val_loss: 0.0164 - val_accuracy: 0.9957 - val_f1_m: 0.9911 - val_precision_m: 0.9878 - val_recall_m: 0.9956- accuracy: 0.9 - ETA: 29s - loss: 0.0429 - accuracy: 0.9846 - f1_m: 0.9690 - preci - ETA: 28s - loss: 0.0428 - accuracy: 0.9846 - f1_m: 0.9691 - precision_m: 0.9795 - recall_m: 0.9 - ETA: 27s - loss: 0.0428 - accuracy: 0.9846 - f1_m: 0.9691 - precision_m: 0.9795 -  - ETA: 27s - l - ETA: 21s - loss: 0.0427 - accuracy: 0.9847 - f1_m: 0.9691 - precision_ - ETA: 20s - loss: 0.0427 - accuracy: 0.9847 - f1_m: 0.9691 - precision_m: 0.9797 - ETA: 19s - loss: 0.0426 - accuracy: 0.9847 - f1_m: 0.9691 - precision_m: 0.9797 - recall_m: - ETA: 18s - loss: 0.0426 - accurac - ETA: 15s - loss: 0.0425 - accuracy: 0.9847 - f1_m: 0.9691 - pr - ETA: 13s - loss: 0.0425 - accuracy: 0.9847 - f1_m: 0.9691 - precision_ - ETA: 11s - loss: 0.0425 - accuracy: 0.9847 - f - ETA: 9s - los - ETA: 5s - loss: 0.0425 - ac\n",
      "Epoch 7/10\n",
      "265622/265622 [==============================] - 481s 2ms/sample - loss: 0.0415 - accuracy: 0.9855 - f1_m: 0.9705 - precision_m: 0.9816 - recall_m: 0.9634 - val_loss: 0.0151 - val_accuracy: 0.9967 - val_f1_m: 0.9931 - val_precision_m: 0.9915 - val_recall_m: 0.9956TA: 2:49 - loss: 0.0422 - accuracy: 0.9852 - f1_m: 0.9700 - precision_m: 0.9814 - recall_m:  - - ETA - ETA: 2:42 - loss: 0.0422 - accuracy: 0.9852 - f1_m: 0.9700 - precision_m: 0 - E - ETA: 1:36 - loss: 0.0419 - accura - ETA: 1:28 - ETA: 1:26 - loss: 0.0418 - accuracy: 0.9853 - f1_m: 0.9703 - precision_m: 0.9816 - recall_m - ETA: 1:26 - loss: 0.0418 - accuracy: 0.9853 - f1_m: 0.9702 - precision_m: 0.9816  - ETA: 1:25 - loss: 0.0418 - accuracy: 0.9853 - f1_m: 0.9702 - precision_m: - ETA: 1:25 - loss: 0.0418 - accuracy: 0.9853 - f1_m: 0.9702 - prec - ETA: 1:07 - loss: 0.0419  - ETA: 1:05 - loss: 0.0420 - accuracy: 0. - ETA: 1:03 - loss: 0.0419 - accuracy: 0.98 - ETA: 49s - loss: 0.0420 - accuracy: 0.9853 - ETA: 40s - loss: 0.0419 - accuracy: 0.9853 - f1_m: 0.9704 - precision_m: 0.9815 - recall_m: 0.963 - ETA: 40s - loss: 0.0419 - accuracy: 0.9853 - f1_m: 0.9704 - precision_m: 0.9815 - recall_m: 0.96 - ETA: 40s - loss: 0.0419 - accuracy: 0.9853 - f1_m: 0.9704 - precision_m: 0.9815 - reca - ETA: 39s - loss: 0.0419 - accuracy: 0.9 - ETA: 36s - loss: 0.0419 - accu - ETA: 32s - loss: 0.0418 - - ETA: 28s - loss: 0.0419 - accuracy: 0 - E - ETA: 18s - loss: 0.0418 - accuracy: 0.9854 - f1_m: 0.9704 - precision_m: 0.9814 - recall_m:  - ETA: 18s - loss: 0.0418 - accuracy: 0.9854 - f1_m: 0.9704 - precision_m: 0.9814 - recall_m:  - ETA: 18s - loss: 0.0418 - accuracy: 0.9854 - f1_m: 0.9703 - precision_m:  - ETA: 17s - loss: 0.0418 - accuracy: 0.9854 - f1_m: 0.97 - ETA: 7s - loss: 0.0416 - accuracy: 0.9854 - f1_m: 0.9705 - precision_m: 0.9814 - re - ETA: 6s - loss: 0.0416 - accuracy: 0.9854 - f1_m: 0.970 - ETA: 0s - loss: 0.0415 - accuracy: 0.9855 - f1_m: 0.9705 - precision_m: 0.9815 - recall_m\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265622/265622 [==============================] - 514s 2ms/sample - loss: 0.0398 - accuracy: 0.9858 - f1_m: 0.9705 - precision_m: 0.9809 - recall_m: 0.9640 - val_loss: 0.0150 - val_accuracy: 0.9966 - val_f1_m: 0.9929 - val_precision_m: 0.9913 - val_recall_m: 0.9956.0386 - accuracy:  - ETA: 5:49 - loss: 0.0390 - accu - ETA: 5:47 - loss: 0 - ETA: 5:37 - loss: 0.0399 - accuracy: 0.9856 - f1_m: 0.9707 - precision_m: 0.9810 - recall - ETA: 5:37 - loss: 0.0398 - accuracy: 0.9856 - f1_m: 0.9707 - precisio - ETA: 5:38 - loss: 0.0399 - accuracy: 0.9856 - f1_m: 0.9 - ETA: 5:37 - loss: 0.0399 - accuracy: 0.9856 - f1_m: 0.9708 - precision_m: 0.9812 - recall_m:  - ETA: 4:41 - loss: 0.0402 - accuracy: 0.9858 - f1_m: 0.9707 - precision_m: 0.9 - ETA: 4:41 - loss: 0.0403 - accuracy: 0.9858 - f1_m: 0.9707 - precision_m: 0.9808 -  - ETA: 3:25 - loss: 0.0404 - accuracy: 0.9858 - f1_ - ETA: 3:24 - loss: 0.0403 - accuracy: 0.9858 - f1_m: 0 - ETA: 3:23 - loss: 0.0403 - accuracy: 0. - ETA: 3:18 - loss: 0.0401 - accuracy: 0.9859 - f1_m: 0.9710 - precision_m: 0.9812 - reca - ETA: 3:18 - loss: - ETA: 3:16 - loss: 0.0401 - accuracy: 0.9859 - f1_m: 0.9710 - precision_m: 0.9812 - recall - ETA: 3:15 - loss: 0.0401 - accura - ETA: 3:13 - loss: 0.0400 - accuracy: 0.9859 - f1_m: 0.9710 - precision_m: 0.9812 - recall_m: 0.96 - ETA: 3:08 - loss: 0.0401 - accuracy: 0.9859 - f1_m: 0.9709 - precision_ - ETA: 2:47 - loss: 0.040 - ETA: 2:45 - loss: 0.0401 - accuracy: 0.9858 - f1_m: 0.9708 -  - ETA: 2:44 - loss: 0.0401 - accuracy: 0.9858 - f1_m: 0.9707 - precision_m: 0.9 - ETA: 2:43 - - ETA: 2:39 - loss: 0.0402 - accu - ETA: 2:26 - - ETA: 1:23 - los - ETA: 1:20 - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9706 - precision_m: 0.9809 - recall_m: 0.96 - ETA: 1:20 - loss: 0.0401 - accuracy - ETA: 1:16 - loss: 0.0402 - accuracy - ETA:  - ETA:  - ETA: 1:09 - loss: 0.0401 - accuracy: 0.9857 - f1_m: - ETA: 1:07 - loss: 0.0401 -  - ETA: 1:05 - loss: 0.0401 - accura - ETA: 1:00 - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9706 - precis - ETA: 59s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9706 - precision_m: 0.9808 - recall_m - ETA: 59s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9705 - precision - ETA: 57s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9705 - pre - ETA: 55s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9706 - prec - ETA: 53s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9706 - precision_m: 0.9808 - recall_m: 0.964 - ETA: 53s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9706 - precision_m: - ETA: 51s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9705 - - ETA: 49s - loss: 0.0402 - accuracy: 0.9857 - f1_m: 0.9705 - precision_m: 0.9808 - recall_m: - ETA: 49s - loss: 0.0402 - accuracy: 0.9857 - f1_m: 0.9705 - precision_m: 0.9808 - recall_m - ETA: 48s - loss: 0.0402 - accuracy: 0.9857 - f1_m: 0.9705 - pr - ETA: 46s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9 - ETA: 44s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9705 - precision_m: 0.9808 - recall_m: 0 - ETA: 44s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9705 - precision_m: - ETA: 42s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9705 - precision_m: 0.9808 - rec - ETA: 41s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9704 - precision_m: 0.9 - ETA: 40s - loss: 0.0402 - accuracy: 0.9857 - f1_m: 0.9704 - prec - ETA: 38s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9704 - precision_m: 0.9807 - recall_ - ETA: 38s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9704 - precision_m: 0.9807 - r - ETA: 37s - loss: 0.0401 - accuracy: 0.9857 - f1_m: 0.9704 - precision_m: 0.9808 - ETA: 36s - loss: 0.0401 - accuracy:  -  - ETA: 26s - loss: 0.0400 - accuracy: 0.9857 - f1_m: - ETA: 23s - loss: 0.0400 - ETA: 2s - loss: 0.0398 - accuracy: 0.9858 - f - ETA: 0s - loss: 0.0398 - accuracy: 0.9858 - f1_m: 0.9705 - precision_m: 0.980 - ETA: 0s - loss: 0.0398 - accuracy: 0.9858 - f1_m: 0.9705 - precision_m: 0.9808 - recall_m - ETA: 0s - loss: 0.0398 - accuracy: 0.9858 - f1_m: 0.9705 - precision_m: 0.9808 - recall_m: \n",
      "Epoch 9/10\n",
      "265622/265622 [==============================] - 463s 2ms/sample - loss: 0.0394 - accuracy: 0.9861 - f1_m: 0.9712 - precision_m: 0.9818 - recall_m: 0.9645 - val_loss: 0.0132 - val_accuracy: 0.9964 - val_f1_m: 0.9925 - val_precision_m: 0.9904 - val_recall_m: 0.9956\n",
      "Epoch 10/10\n",
      "265622/265622 [==============================] - 481s 2ms/sample - loss: 0.0382 - accuracy: 0.9864 - f1_m: 0.9719 - precision_m: 0.9820 - recall_m: 0.9657 - val_loss: 0.0136 - val_accuracy: 0.9971 - val_f1_m: 0.9938 - val_precision_m: 0.9929 - val_recall_m: 0.9956\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss = 'binary_crossentropy', metrics=['accuracy',f1_m,precision_m, recall_m])\n",
    "history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013647656919130221 0.997066 0.993842 0.9929464 0.99556285\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
    "print (loss,accuracy,f1_score,precision,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
